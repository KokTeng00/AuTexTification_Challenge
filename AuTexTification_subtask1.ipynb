{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "h-Gv_IJeZokd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1787,
     "status": "ok",
     "timestamp": 1682279828348,
     "user": {
      "displayName": "NG KOK TENG",
      "userId": "06944301600455144405"
     },
     "user_tz": -120
    },
    "id": "h-Gv_IJeZokd",
    "outputId": "354513f2-d5fa-4fa3-a483-f01cacddd84a"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "962c0f2f",
   "metadata": {
    "id": "962c0f2f"
   },
   "outputs": [],
   "source": [
    "#To set the seed for ensuring the generated result will be exactly same in every execution\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "torch.manual_seed(RANDOM_SEED)\n",
    "torch.cuda.manual_seed_all(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a772b875",
   "metadata": {
    "id": "a772b875"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#For reading the training dataset\n",
    "sub_1 = pd.read_csv('train.tsv', sep = '\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36343124",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1682279832713,
     "user": {
      "displayName": "NG KOK TENG",
      "userId": "06944301600455144405"
     },
     "user_tz": -120
    },
    "id": "36343124",
    "outputId": "a46fa279-8e71-480b-f02d-72a405f319bd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>12322</td>\n",
       "      <td>you need to stop the engine and wait until it ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1682</td>\n",
       "      <td>The Commission shall publish the report; an in...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>22592</td>\n",
       "      <td>I have not been tweeting a lot lately, but I d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>17390</td>\n",
       "      <td>I pass my exam and really thankgod for that bu...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>30453</td>\n",
       "      <td>The template will have 3 parts: a mustache sha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id                                               text  label\n",
       "0           0  12322  you need to stop the engine and wait until it ...      0\n",
       "1           1   1682  The Commission shall publish the report; an in...      0\n",
       "2           2  22592  I have not been tweeting a lot lately, but I d...      0\n",
       "3           3  17390  I pass my exam and really thankgod for that bu...      1\n",
       "4           4  30453  The template will have 3 parts: a mustache sha...      1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#View the dataset for the first five rows\n",
    "sub_1.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a3aca86",
   "metadata": {
    "id": "4a3aca86"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "#Transforming the 'label' variable, could easily process by machine\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "sub_1['label'] = label_encoder.fit_transform(sub_1['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "K3Z7eE13Sy1U",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 521,
     "status": "ok",
     "timestamp": 1682279841414,
     "user": {
      "displayName": "NG KOK TENG",
      "userId": "06944301600455144405"
     },
     "user_tz": -120
    },
    "id": "K3Z7eE13Sy1U",
    "outputId": "89b35fac-879f-4aa3-da2a-75dc33af537e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27414 3385 3046\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "#Splitting the data into train and test, and using the stratified methods to ensure the data is splitted distributed equally based on the dependent variable\n",
    "train, test = train_test_split(sub_1, test_size=0.10, stratify=sub_1['label']) \n",
    "train, validation = train_test_split(train, test_size=0.10, stratify=train['label']) \n",
    "\n",
    "print(len(train), len(test), len(validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d_VMGa0CavuH",
   "metadata": {
    "id": "d_VMGa0CavuH"
   },
   "outputs": [],
   "source": [
    "train_text = list(train['text'])\n",
    "train_label = list(train['label'])\n",
    "\n",
    "val_text = list(validation['text'])\n",
    "val_label = list(validation['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27d17993",
   "metadata": {
    "id": "27d17993"
   },
   "outputs": [],
   "source": [
    "#Set up the tokenizer from the BERT neural network\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "#bert-base-uncased means it will treat upper case and lower case same as 'english' and 'English'\n",
    "PRE_TRAINED_MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "aUyfuoK_o77a",
   "metadata": {
    "id": "aUyfuoK_o77a"
   },
   "outputs": [],
   "source": [
    "token_counts = []\n",
    "\n",
    "#For finding the token length of the text after executing the process of tokenisation, then we could determine the maximum length of the tokenisation\n",
    "for text in train_text:\n",
    "  token_count = len(tokenizer.encode(text, max_length = 512, truncation = True))\n",
    "  token_counts.append(token_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8PNV6N7mlm-j",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 447
    },
    "executionInfo": {
     "elapsed": 906,
     "status": "ok",
     "timestamp": 1682079281015,
     "user": {
      "displayName": "NG KOK TENG",
      "userId": "16553334513796632904"
     },
     "user_tz": -120
    },
    "id": "8PNV6N7mlm-j",
    "outputId": "d3c7cefe-1f05-4d49-e953-c34b1f59fd6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 512.0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#For plotting the histogram of the token length of the texts after executing the process of tokenisation\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.histplot(token_counts)\n",
    "plt.xlim([0, 512])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GGGK9Md8r7-z",
   "metadata": {
    "id": "GGGK9Md8r7-z"
   },
   "source": [
    "Based on the result shown above, we could view the tokens' input_ids lengths are fallen within the range of (0, 128), which mean the input sentences's length are falling within these range. Therefore, we could set the maximum length for the tokenisation be 128, which could save more computation time and space."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15df658",
   "metadata": {},
   "source": [
    "# Translating Foreign Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abb785c3",
   "metadata": {},
   "source": [
    "###### Could uncomment it to run it (Due to it got poor performance compare to non-translating situation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c539e7d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "!pip install pyicu\n",
    "!pip install pycld2\n",
    "!pip install polyglot\n",
    "!pip install morfessor\n",
    "!pip install googletrans==4.0.0rc1\n",
    "\n",
    "import chardet\n",
    "from polyglot.detect import Detector\n",
    "\n",
    "foreign_language = []\n",
    "\n",
    "for length_of_sub_1 in range(len(sub_1)):\n",
    "  text = sub_1['text'][length_of_sub_1]\n",
    "  try:\n",
    "    detector = Detector(text)\n",
    "    if (detector.language.confidence < 98.0):\n",
    "      foreign_language.append(length_of_sub_1)\n",
    "  except:\n",
    "    detect = chardet.detect(text.encode())\n",
    "    if (detect['confidence'] < 0.99):\n",
    "      foreign_language.append(length_of_sub_1)\n",
    "    else:\n",
    "      pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214e7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import nltk\n",
    "import time\n",
    "nltk.download(\"punkt\")\n",
    "from googletrans import Translator\n",
    "from nltk.tokenize import WordPunctTokenizer\n",
    "\n",
    "tk = WordPunctTokenizer()\n",
    "translator = Translator()\n",
    "\n",
    "for i in range (len(foreign_language) // 2):\n",
    "  text = sub_1['text'][foreign_language[i]]\n",
    "  translation = translator.translate(text, dest = 'en')\n",
    "  text = translation.text\n",
    "  time.sleep(0.5)\n",
    "  sub_1['text'][foreign_language[i]] = text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5293514c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tk = WordPunctTokenizer()\n",
    "translator = Translator()\n",
    "\n",
    "for i in range (len(foreign_language) // 2, len(foreign_language)):\n",
    "  text = sub_1['text'][foreign_language[i]]\n",
    "  translation = translator.translate(text, dest = 'en')\n",
    "  text = translation.text\n",
    "  time.sleep(0.5)\n",
    "  sub_1['text'][foreign_language[i]] = text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1275f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "foreign_language = []\n",
    "\n",
    "for length_of_sub_1 in range(len(sub_1)):\n",
    "  text = sub_1['text'][length_of_sub_1]\n",
    "  try:\n",
    "    detector = Detector(text)\n",
    "    if (detector.language.confidence < 98.0):\n",
    "      foreign_language.append(length_of_sub_1)\n",
    "  except:\n",
    "    detect = chardet.detect(text.encode())\n",
    "    if (detect['confidence'] < 0.99):\n",
    "      foreign_language.append(length_of_sub_1)\n",
    "    else:\n",
    "      pass\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3f86a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "tk = WordPunctTokenizer()\n",
    "translator = Translator()\n",
    "\n",
    "for i in range (len(foreign_language) // 2):\n",
    "  text = sub_1['text'][foreign_language[i]]\n",
    "  tokens = tk.tokenize(text)\n",
    "  for length_of_token in range (len(tokens)):\n",
    "    translation = translator.translate(tokens[length_of_token], dest = 'en')\n",
    "    tokens[length_of_token] = translation.text\n",
    "    time.sleep(0.5)\n",
    "  text = ' '.join(tokens)\n",
    "  sub_1['text'][foreign_language[i]] = text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd32e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "for i in range (len(foreign_language)//2, len(foreign_language)):\n",
    "  text = sub_1['text'][foreign_language[i]]\n",
    "  tokens = tk.tokenize(text)\n",
    "  for length_of_token in range (len(tokens)):\n",
    "    translation = translator.translate(tokens[length_of_token], dest = 'en')\n",
    "    tokens[length_of_token] = translation.text\n",
    "    time.sleep(0.5)\n",
    "  text = ' '.join(tokens)\n",
    "  sub_1['text'][foreign_language[i]] = text\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "QKBBqGa4l9bm",
   "metadata": {
    "id": "QKBBqGa4l9bm"
   },
   "source": [
    "# Searching Hyperparameter for BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "W-Sh_K9iWVKJ",
   "metadata": {
    "id": "W-Sh_K9iWVKJ"
   },
   "outputs": [],
   "source": [
    "import emoji\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \n",
    "    @classmethod\n",
    "    def convert_emoticons(cls, text):\n",
    "        for emot in EMOTICONS_EMO:\n",
    "            text = text.replace(emot, EMOTICONS_EMO[emot].replace(\" \",\"_\"))\n",
    "        return text\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess_text(cls, texts):\n",
    "        preprocessed_texts = []\n",
    "        for text in texts:\n",
    "            text = emoji.demojize(text)\n",
    "            text = cls.convert_emoticons(text)\n",
    "            preprocessed_texts.append(text)\n",
    "\n",
    "        return preprocessed_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afc1756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing on those sentence \n",
    "\"\"\"\n",
    "import emoji\n",
    "from nltk import pos_tag\n",
    "from torch import nn, optim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "class TextPreprocessor:\n",
    "    tk = WordPunctTokenizer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    @classmethod\n",
    "    def convert_emoticons(cls, text):\n",
    "        for emot in EMOTICONS_EMO:\n",
    "            text = text.replace(emot, EMOTICONS_EMO[emot].replace(\" \",\"_\"))\n",
    "        return text\n",
    "\n",
    "    @classmethod\n",
    "    def get_wordnet_pos(cls, treebank_tag):\n",
    "        if treebank_tag.startswith('J'):\n",
    "            return wordnet.ADJ\n",
    "        elif treebank_tag.startswith('V'):\n",
    "            return wordnet.VERB\n",
    "        elif treebank_tag.startswith('N'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('R'):\n",
    "            return wordnet.ADV\n",
    "        elif treebank_tag.startswith('C'):\n",
    "            return wordnet.NOUN\n",
    "        elif treebank_tag.startswith('M'):\n",
    "            return 'v'\n",
    "        elif treebank_tag.startswith('I'):\n",
    "            return wordnet.ADV\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    @classmethod\n",
    "    def preprocess_text(cls, texts):\n",
    "        preprocessed_texts = []\n",
    "        for text in texts:\n",
    "            text = emoji.demojize(text)\n",
    "            text = cls.convert_emoticons(text)\n",
    "            tokens = cls.tk.tokenize(text)\n",
    "            tokens = [token for token in tokens if token not in cls.stop_words]\n",
    "            tokens = [token for token in tokens if token.isalnum()]\n",
    "            tokens = [token for token in tokens if not isinstance(token, str) or not token.isnumeric()]\n",
    "            pos_tags = pos_tag(tokens)\n",
    "            lemmatized_tokens = [cls.lemmatizer.lemmatize(token, pos = cls.get_wordnet_pos(tag)) if cls.get_wordnet_pos(tag) is not None else token for token, tag in pos_tags]\n",
    "            preprocessed_texts.append(lemmatized_tokens)\n",
    "\n",
    "        return preprocessed_texts\n",
    "\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "Oy-kDzrx83d2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122,
     "referenced_widgets": [
      "3d865d67baea4844ad5760719f532c3a",
      "d4dd7dfcef994f48a2cd58bb13a2c064",
      "6181eea583f0425e88b106f06362966f",
      "73285c1ef82f4eed8ebf6857d12d068c",
      "89df390f4d3c419ba58b25999dc7d59e",
      "a58d3ae47a774fafa87377803147fab6",
      "00492269a6e944109537c73c562e3432",
      "101bde3c0df94217ba6b1615c211d940",
      "fef9d31373c745a98c142273292e87d6",
      "7582db9de64f475c8ec37d1927a3ee70",
      "ab7c027542454bdfbb5a554cee3cd729",
      "0ba494e7fa364dae93196c13ba0144c3",
      "04167c969142405bb0d754fc01bdcb6b",
      "4de519c880024ba58f22ce5116a95178",
      "d1db0ccc0c3c494690da726828d6705f",
      "f1b69d16f76a43c7b4e37ab618824ba0",
      "fb9087bdada9477691cbc4c4cd38b2e0",
      "c21953075c514b9ca632347105d2ff82",
      "26b5bda23f9b4cf58f5c24e09e0c32ff",
      "c6a485c9c5df41368dc734414cbcc86b",
      "903a6925e28949d4b8c75e80d95800a9",
      "b3f56b71464842ac8832425afbf8253a"
     ]
    },
    "executionInfo": {
     "elapsed": 62781,
     "status": "ok",
     "timestamp": 1682278470522,
     "user": {
      "displayName": "NG KOK TENG",
      "userId": "06944301600455144405"
     },
     "user_tz": -120
    },
    "id": "Oy-kDzrx83d2",
    "outputId": "0be0ba94-0fcd-4d90-d4e3-e97a99bc6ef4"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3046 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "#train_text = TextPreprocessor.preprocess_text(train_text)\n",
    "#val_text = TextPreprocessor.preprocess_text(val_text)\n",
    "\n",
    "train = pd.DataFrame({'text': train_text, 'label': train_label})\n",
    "val = pd.DataFrame({'text': val_text, 'label': val_label})\n",
    "\n",
    "train = datasets.Dataset.from_pandas(train)\n",
    "val = datasets.Dataset.from_pandas(val)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME) \n",
    "\n",
    "def preprocess(examples):   \n",
    "# This will tokenize the text, add padding or truncate the text to the max length of 128     \n",
    "    return tokenizer(examples['text'],truncation=True,   padding='max_length',max_length=128)   \n",
    "\n",
    "train = train.map(preprocess, batched=True)\n",
    "val = val.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "lCOeD6IHYK-n",
   "metadata": {
    "id": "lCOeD6IHYK-n",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-05-05 22:22:38,327]\u001b[0m A new study created in memory with name: no-name-35d94920-7767-4d4b-a933-705150d4d04f\u001b[0m\n",
      "/tmp/ipykernel_49378/2920327591.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", low=1e-6, high=1e-4)\n",
      "/tmp/ipykernel_49378/2920327591.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", low=1e-6, high=1e-4)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.7286, 'learning_rate': 2.040042647415444e-07, 'epoch': 0.04}\n",
      "{'loss': 0.7214, 'learning_rate': 4.080085294830888e-07, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[W 2023-05-05 22:22:55,595]\u001b[0m Trial 0 failed with parameters: {'learning_rate': 2.1134841827224e-05, 'weight_decay': 2.985396672423869e-06, 'num_train_epochs': 1, 'eval_steps': 889, 'warmup_steps': 518} because of the following error: KeyboardInterrupt().\u001b[0m\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wifo3tp01/miniconda3/envs/authentication/lib/python3.11/site-packages/optuna/study/_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_49378/2920327591.py\", line 44, in objective\n",
      "    trainer.train()\n",
      "  File \"/home/wifo3tp01/miniconda3/envs/authentication/lib/python3.11/site-packages/transformers/trainer.py\", line 1662, in train\n",
      "    return inner_training_loop(\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wifo3tp01/miniconda3/envs/authentication/lib/python3.11/site-packages/transformers/trainer.py\", line 1929, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/wifo3tp01/miniconda3/envs/authentication/lib/python3.11/site-packages/transformers/trainer.py\", line 2717, in training_step\n",
      "    loss.backward()\n",
      "  File \"/home/wifo3tp01/miniconda3/envs/authentication/lib/python3.11/site-packages/torch/_tensor.py\", line 487, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/wifo3tp01/miniconda3/envs/authentication/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 200, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "\u001b[33m[W 2023-05-05 22:22:55,602]\u001b[0m Trial 0 failed with value None.\u001b[0m\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     50\u001b[0m     study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m     study\u001b[38;5;241m.\u001b[39moptimize(objective, n_trials \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m25\u001b[39m)\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest trial: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstudy\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/authentication/lib/python3.11/site-packages/optuna/study/study.py:425\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    323\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    330\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    331\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    333\u001b[0m \n\u001b[1;32m    334\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    423\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 425\u001b[0m     _optimize(\n\u001b[1;32m    426\u001b[0m         study\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    427\u001b[0m         func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m    428\u001b[0m         n_trials\u001b[38;5;241m=\u001b[39mn_trials,\n\u001b[1;32m    429\u001b[0m         timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m    430\u001b[0m         n_jobs\u001b[38;5;241m=\u001b[39mn_jobs,\n\u001b[1;32m    431\u001b[0m         catch\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mtuple\u001b[39m(catch) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(catch, Iterable) \u001b[38;5;28;01melse\u001b[39;00m (catch,),\n\u001b[1;32m    432\u001b[0m         callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m    433\u001b[0m         gc_after_trial\u001b[38;5;241m=\u001b[39mgc_after_trial,\n\u001b[1;32m    434\u001b[0m         show_progress_bar\u001b[38;5;241m=\u001b[39mshow_progress_bar,\n\u001b[1;32m    435\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/authentication/lib/python3.11/site-packages/optuna/study/_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 66\u001b[0m         _optimize_sequential(\n\u001b[1;32m     67\u001b[0m             study,\n\u001b[1;32m     68\u001b[0m             func,\n\u001b[1;32m     69\u001b[0m             n_trials,\n\u001b[1;32m     70\u001b[0m             timeout,\n\u001b[1;32m     71\u001b[0m             catch,\n\u001b[1;32m     72\u001b[0m             callbacks,\n\u001b[1;32m     73\u001b[0m             gc_after_trial,\n\u001b[1;32m     74\u001b[0m             reseed_sampler_rng\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     75\u001b[0m             time_start\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     76\u001b[0m             progress_bar\u001b[38;5;241m=\u001b[39mprogress_bar,\n\u001b[1;32m     77\u001b[0m         )\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/miniconda3/envs/authentication/lib/python3.11/site-packages/optuna/study/_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m _run_trial(study, func, catch)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/miniconda3/envs/authentication/lib/python3.11/site-packages/optuna/study/_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    250\u001b[0m ):\n\u001b[0;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/miniconda3/envs/authentication/lib/python3.11/site-packages/optuna/study/_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m func(trial)\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[17], line 44\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# Define trainer and train model\u001b[39;00m\n\u001b[1;32m     37\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     38\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     39\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m     40\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtrain_dataset,\n\u001b[1;32m     41\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mval_dataset\n\u001b[1;32m     42\u001b[0m )\n\u001b[0;32m---> 44\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Return validation loss as the objective to minimize\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mevaluate(val_dataset)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meval_loss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/authentication/lib/python3.11/site-packages/transformers/trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1661\u001b[0m )\n\u001b[0;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[1;32m   1663\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   1664\u001b[0m     resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[1;32m   1665\u001b[0m     trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[1;32m   1666\u001b[0m     ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[1;32m   1667\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/authentication/lib/python3.11/site-packages/transformers/trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1927\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1935\u001b[0m ):\n\u001b[1;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/authentication/lib/python3.11/site-packages/transformers/trainer.py:2717\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2715\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[1;32m   2716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2717\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[0;32m~/miniconda3/envs/authentication/lib/python3.11/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/authentication/lib/python3.11/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", low=1e-6, high=1e-4)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", low=1e-6, high=1e-4)\n",
    "    num_train_epochs = trial.suggest_categorical(\"num_train_epochs\", [1, 2, 3, 5])\n",
    "    eval_steps = trial.suggest_int(\"eval_steps\", low=10, high=1000)\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", low=10, high=1000)\n",
    "\n",
    "    # Load BERT model and tokenizer\n",
    "    model = BertForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels = 2)\n",
    "\n",
    "    # Load train and validation datasets\n",
    "    train_dataset = train\n",
    "    val_dataset = val\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=256,\n",
    "        per_device_eval_batch_size=256,\n",
    "        warmup_steps=warmup_steps,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=5,\n",
    "        evaluation_strategy='epoch',\n",
    "        eval_steps=eval_steps,\n",
    "        disable_tqdm=True\n",
    "    )\n",
    "\n",
    "    # Define trainer and train model\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Return validation loss as the objective to minimize\n",
    "    return trainer.evaluate(val_dataset)['eval_loss']\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials = 25)\n",
    "    print(f\"Best trial: {study.best_trial.params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ee56c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'learning_rate': 2.648756253220067e-05, 'weight_decay': 3.589471506155498e-05, 'num_train_epochs': 2, 'eval_steps': 108, 'warmup_steps': 308}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best trial: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1CCIUulVnIKn",
   "metadata": {
    "id": "1CCIUulVnIKn"
   },
   "source": [
    "# Fine-Tune Bert Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "zWdgf1QpraV8",
   "metadata": {
    "id": "zWdgf1QpraV8"
   },
   "outputs": [],
   "source": [
    "from torch import nn, optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "#This class is used to execute the preprocessing purpose\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    def __init__(self, text, labels, tokenizer, max_len):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def convert_emoticons(self, text):\n",
    "      for emot in EMOTICONS_EMO:\n",
    "          text = text.replace(emot, EMOTICONS_EMO[emot].replace(\" \",\"_\"))\n",
    "      return text\n",
    "\n",
    "    #For tokenize the sentence by using encode_plus() function, it could generate the input_ids and attention_mask which are needed to fit into BERT pre-trained model for fine-tuning.\n",
    "    #In addition, encode_plus() is more flexible than encode() as able to setting more parameters\n",
    "    def __getitem__(self, item):\n",
    "      text = str(self.text[item])\n",
    "      label = self.labels[item]\n",
    "      #text = [emoji.demojize(sentences) for sentences in text]\n",
    "      #text = [self.convert_emoticons(sentences) for sentences in text]\n",
    "      encoding = self.tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_len,\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        return_token_type_ids=False, #Segmentation embedding\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt',\n",
    "      )\n",
    "\n",
    "    #After executing this preprocessing class, it will return the original text, input_ids, attention_mask, and the label of the text\n",
    "      return {\n",
    "        'input_ids': encoding['input_ids'].flatten(),\n",
    "        'attention_mask': encoding['attention_mask'].flatten(),\n",
    "        'labels': torch.tensor(label, dtype=torch.long)\n",
    "      }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38cdd94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario for preprocessing \n",
    "\"\"\"\n",
    "import emoji\n",
    "from nltk import pos_tag\n",
    "from torch import nn, optim\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from emot.emo_unicode import EMOTICONS_EMO\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "#This class is used to execute the preprocessing purpose\n",
    "class TextDataset(Dataset):\n",
    "\n",
    "    tk = WordPunctTokenizer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "    def __init__(self, text, labels, tokenizer, max_len):\n",
    "        self.text = text\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def convert_emoticons(self, text):\n",
    "      for emot in EMOTICONS_EMO:\n",
    "          text = text.replace(emot, EMOTICONS_EMO[emot].replace(\" \",\"_\"))\n",
    "      return text\n",
    "\n",
    "    def get_wordnet_pos(self, treebank_tag):\n",
    "      if treebank_tag.startswith('J'): # Adjective (JJ)\n",
    "          return wordnet.ADJ\n",
    "      elif treebank_tag.startswith('V'): # Verb (VB)\n",
    "          return wordnet.VERB\n",
    "      elif treebank_tag.startswith('N'): # Noun (NN)\n",
    "          return wordnet.NOUN\n",
    "      elif treebank_tag.startswith('R'): # Adverb (RB) \n",
    "          return wordnet.ADV\n",
    "      elif treebank_tag.startswith('C'): # Conjunction (CC)\n",
    "          return wordnet.NOUN\n",
    "      elif treebank_tag.startswith('M'):\n",
    "          return 'v'\n",
    "      elif treebank_tag.startswith('I'): # Preposition (IN)\n",
    "          return wordnet.ADV\n",
    "      else:\n",
    "          return None\n",
    "\n",
    "    #For tokenize the sentence by using encode_plus() function, it could generate the input_ids and attention_mask which are needed to fit into BERT pre-trained model for fine-tuning.\n",
    "    #In addition, encode_plus() is more flexible than encode() as able to setting more parameters\n",
    "    def __getitem__(self, item):\n",
    "        text = str(self.text[item])\n",
    "        label = self.labels[item]\n",
    "        text = [emoji.demojize(sentences) for sentences in text]\n",
    "        text = [self.convert_emoticons(sentences) for sentences in text]\n",
    "        text = [self.tk.tokenize(sentences) for sentences in text]\n",
    "        text = [[token for token in sentence if token not in self.stop_words] for sentence in text]\n",
    "        text = [[token for token in sentence if token.isalnum()] for sentence in text]\n",
    "        text = [[token for token in sentence if not isinstance(token, str) or not token.isnumeric()] for sentence in text]\n",
    "        text = [pos_tag(sentence) for sentence in text]\n",
    "        text = [[self.lemmatizer.lemmatize(token, pos=get_wordnet_pos(tag)) if get_wordnet_pos(tag) is not None else token for token, tag in sentence] for sentence in text]        \n",
    "        text = [' '.join(sentence) for sentence in text]\n",
    "        text = ' '.join(text)\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "          text,\n",
    "          add_special_tokens=True,\n",
    "          max_length=self.max_len,\n",
    "          truncation=True,\n",
    "          padding='max_length',\n",
    "          return_token_type_ids=False, #Segmentation embedding\n",
    "          return_attention_mask=True,\n",
    "          return_tensors='pt',\n",
    "        )\n",
    "\n",
    "        #After executing this preprocessing class, it will return the input_ids, attention_mask, and the label of the text\n",
    "        return {\n",
    "          #'text': text,\n",
    "          'input_ids': encoding['input_ids'].flatten(),\n",
    "          'attention_mask': encoding['attention_mask'].flatten(),\n",
    "          'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "984R_2F_Q6yR",
   "metadata": {
    "id": "984R_2F_Q6yR"
   },
   "outputs": [],
   "source": [
    "#For executing the Neural Network, the input data should be using the DataLoader() function to split them into multiple batches\n",
    "def create_data_loader(X, Y, tokenizer, max_len, batch_size, num_workers=2, sampler = None):\n",
    "    ds = TextDataset(\n",
    "    text = np.array(X),\n",
    "    labels = np.array(Y),\n",
    "    tokenizer = tokenizer,\n",
    "    max_len = max_len\n",
    "  )\n",
    "    if sampler != None:\n",
    "        sampler = sampler(ds)\n",
    "\n",
    "    return DataLoader(\n",
    "        ds,\n",
    "        batch_size = batch_size,\n",
    "        num_workers = num_workers,\n",
    "        sampler = sampler,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "wbzP6-JuRMw9",
   "metadata": {
    "id": "wbzP6-JuRMw9"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "#Since the input data is too large that consisted by 33845 rows of sentences then we could set the batch size with small value as 16\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_data_loader = create_data_loader(train_text, train_label, tokenizer=tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE, sampler=RandomSampler)\n",
    "val_data_loader = create_data_loader(val_text, val_label, tokenizer=tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE, sampler=SequentialSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "QnD8S76nBwVw",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5656,
     "status": "ok",
     "timestamp": 1682278521370,
     "user": {
      "displayName": "NG KOK TENG",
      "userId": "06944301600455144405"
     },
     "user_tz": -120
    },
    "id": "QnD8S76nBwVw",
    "outputId": "f3aacbff-d934-48ac-8430-19f87e5a3899",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Setting up the BERT model configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    PRE_TRAINED_MODEL_NAME,\n",
    "    num_labels = 2,  \n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "Aq8eNfkpIDVK",
   "metadata": {
    "id": "Aq8eNfkpIDVK"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "#Executing Stochastic Gradient Descent for adjusting the weight based on the error that calculated by using the actual label and predicted label\n",
    "#The learning rate for Stochastic Gradient Descent is 0.000001\n",
    "optimizer = AdamW(model.parameters(), lr = 2.648756253220067e-05, weight_decay = 3.589471506155498e-05,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "t2-MYKmRIDXs",
   "metadata": {
    "id": "t2-MYKmRIDXs"
   },
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4 (depend on the usage, you can also set it larger)\n",
    "# We chose to run for 1 epoch first\n",
    "EPOCHS = 2\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler, here we use a linear scheduler with no warmup steps\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 308,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Define our loss function\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6mAGeiqxIDZ_",
   "metadata": {
    "id": "6mAGeiqxIDZ_"
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "def train_epoch(model, data_loader, loss_fn, optimizer, device, scheduler, n_examples):\n",
    "    model.train()\n",
    "    total_train_accuracy = 0\n",
    "    total_train_loss = 0\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    for step, batch in enumerate(data_loader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            print('Batch: {}  of  {}'.format(step, len(data_loader)))\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "        model.zero_grad()\n",
    "        outputs = model(\n",
    "          input_ids=input_ids,\n",
    "          token_type_ids=None,\n",
    "          attention_mask=attention_mask,\n",
    "          labels=labels\n",
    "        )\n",
    "        loss = outputs[0]\n",
    "        total_train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        logits = outputs[1].detach().cpu().numpy()\n",
    "        label_ids = labels.to('cpu').numpy()\n",
    "        total_train_accuracy += flat_accuracy(logits, label_ids)\n",
    "  # Calculate the average loss over all of the batches.\n",
    "    avg_train_accuracy = total_train_accuracy / len(data_loader)\n",
    "    avg_train_loss = total_train_loss / len(data_loader) \n",
    "    return avg_train_accuracy, avg_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "If0-dyOiIDcV",
   "metadata": {
    "id": "If0-dyOiIDcV"
   },
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cG2nCQR7IDeu",
   "metadata": {
    "id": "cG2nCQR7IDeu"
   },
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 108\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "            )\n",
    "            total_eval_loss += outputs[0].item()\n",
    "            logits = outputs[1].detach().cpu().numpy()\n",
    "            label_ids = labels.to('cpu').numpy()\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(data_loader)\n",
    "    avg_val_loss = total_eval_loss / len(data_loader)\n",
    "    return avg_val_accuracy, avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7-yGaf8SSRKm",
   "metadata": {
    "id": "7-yGaf8SSRKm",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2\n",
      "----------\n",
      "Batch: 40  of  108\n",
      "Batch: 80  of  108\n",
      "Train loss: 0.5957313163412942, Accuracy: 0.6552175399831649\n",
      "validation loss: 0.40835823367039364, Accuracy: 0.8127292798913044\n",
      "\n",
      "Epoch: 2/2\n",
      "----------\n",
      "Batch: 40  of  108\n",
      "Batch: 80  of  108\n",
      "Train loss: 0.2924591163518252, Accuracy: 0.8791265519781144\n",
      "validation loss: 0.3544682463010152, Accuracy: 0.8443840579710145\n",
      "\n",
      "CPU times: user 4min, sys: 1.16 s, total: 4min 1s\n",
      "Wall time: 4min 5s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch: {}/{}'.format(epoch+1, EPOCHS))\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(train_text)\n",
    "    )\n",
    "    print('Train loss: {}, Accuracy: {}'.format(train_loss, train_acc))\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(val_text)\n",
    "    )\n",
    "    print('validation loss: {}, Accuracy: {}'.format(val_loss, val_acc))\n",
    "    print()\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['validation_acc'].append(val_acc)\n",
    "    history['validation_loss'].append(val_loss)\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4b86f2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('bert model (subtask 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "57a62ac6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained('bert model (subtask 1)')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "648b5583",
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_label = test.copy()\n",
    "\n",
    "extract_label.drop(['text'], axis = 1, inplace = True)\n",
    "extract_label.loc[extract_label['label'] == 0, 'label'] = 'generated'\n",
    "extract_label.loc[extract_label['label'] == 1, 'label'] = 'human'\n",
    "extract_label.set_index('id', inplace = True)\n",
    "extract_label.to_csv('AuTexTificationEval/task_submissions/ground_truth/subtask_1/en/truth.tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e821712d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bert_test = test.copy()\n",
    "\n",
    "# Tokenize test data\n",
    "test_inputs = tokenizer(\n",
    "    bert_test[\"text\"].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=128,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "batch_size = 256\n",
    "num_samples = len(test_inputs[\"input_ids\"])\n",
    "test_preds = []\n",
    "\n",
    "# iterate through batches and make predictions\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    batch_inputs = {\n",
    "        \"input_ids\": test_inputs[\"input_ids\"][i:i+batch_size].to(device),\n",
    "        \"attention_mask\": test_inputs[\"attention_mask\"][i:i+batch_size].to(device)\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        batch_outputs = model(batch_inputs[\"input_ids\"], batch_inputs[\"attention_mask\"])\n",
    "        batch_preds = torch.argmax(batch_outputs.logits, dim=1)\n",
    "        test_preds.extend(batch_preds.cpu().tolist())\n",
    "\n",
    "# concatenate predictions from all batches\n",
    "bert_test[\"label\"] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "38daace1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_test.drop(['text'], axis = 1, inplace = True)\n",
    "bert_test.loc[bert_test['label'] == 0, 'label'] = 'generated'\n",
    "bert_test.loc[bert_test['label'] == 1, 'label'] = 'human'\n",
    "bert_test.set_index('id', inplace = True)\n",
    "bert_test.to_csv('AuTexTificationEval/task_submissions/submissions/my_team/subtask_1/en/pred (bert subtask 1).tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "Z2gwbA1yYLA3",
   "metadata": {
    "id": "Z2gwbA1yYLA3",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15725</td>\n",
       "      <td>It has remained one of my favorite country/swi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17108</td>\n",
       "      <td>Even with very light use (hard to get motivate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>383</td>\n",
       "      <td>She died in 2015 at age 93. She is survived by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7809</td>\n",
       "      <td>Londonderry Crown Court heard how Heaney false...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6215</td>\n",
       "      <td>Will Genia, Lachie Turner and Berrick Barnes e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text\n",
       "0  15725  It has remained one of my favorite country/swi...\n",
       "1  17108  Even with very light use (hard to get motivate...\n",
       "2    383  She died in 2015 at age 93. She is survived by...\n",
       "3   7809  Londonderry Crown Court heard how Heaney false...\n",
       "4   6215  Will Genia, Lachie Turner and Berrick Barnes e..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub_1_test = pd.read_csv('test.tsv', sep = '\\t')\n",
    "display(sub_1_test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ca6c30bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize test data\n",
    "test_inputs = tokenizer(\n",
    "    sub_1_test[\"text\"].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=128,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "batch_size = 256\n",
    "num_samples = len(test_inputs[\"input_ids\"])\n",
    "test_preds = []\n",
    "\n",
    "# iterate through batches and make predictions\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    batch_inputs = {\n",
    "        \"input_ids\": test_inputs[\"input_ids\"][i:i+batch_size].to(device),\n",
    "        \"attention_mask\": test_inputs[\"attention_mask\"][i:i+batch_size].to(device)\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        batch_outputs = model(batch_inputs[\"input_ids\"], batch_inputs[\"attention_mask\"])\n",
    "        batch_preds = torch.argmax(batch_outputs.logits, dim=1)\n",
    "        test_preds.extend(batch_preds.cpu().tolist())\n",
    "\n",
    "# concatenate predictions from all batches\n",
    "sub_1_test[\"label\"] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "2cf91ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_1_test.drop(['text'], axis = 1, inplace = True)\n",
    "sub_1_test.loc[sub_1_test['label'] == 0, 'label'] = 'generated'\n",
    "sub_1_test.loc[sub_1_test['label'] == 1, 'label'] = 'human'\n",
    "sub_1_test.set_index('id', inplace = True)\n",
    "sub_1_test.to_csv('AuTexTificationEval/task_submissions/submissions/my_team/subtask_1/en/run1 (bert).tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "DJoTx8GcpWxh",
   "metadata": {
    "id": "DJoTx8GcpWxh"
   },
   "source": [
    "# Searching Hyperparameter for XLM-R Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "hFV6FeVHpaFB",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17,
     "referenced_widgets": [
      "1c6e0f7004d54fe394ca0afb85a35383",
      "263214a51f0e485fa8c8ea593e0ac76a",
      "b473a2b03ecf422881d04ac476e92573",
      "7a75ff77a75f45208674285b0eee9485",
      "89a59a27e4cb4509a086181edfcdbcf1",
      "7a7756b2e8044da7bb4061d0f80eb5ae",
      "6911acceff924d98bb401626acb272e7",
      "49ef9f5a74cd41d09192e5752a4c4f21",
      "1fb417ca34dc45159aa389e439fbf76e",
      "b0547766af2b4bd9b64f358d5039177c",
      "20434d509bd04840bc04fac21a78b7af",
      "775ddbf71fae4ad3b5d1258401ad706e",
      "88b10b31cfc948ce9087841b71066e8e",
      "a9429551fb9c43ff9ab32ba7948e3891",
      "c68286244b354d47b7d673e781ecf420",
      "ae17f8e3c2d546aab05238a6501606ac",
      "7c7deaf37ba34db0993794e9a43b39bb",
      "5db031a1c6c34bbda3e1aa56125a5419",
      "02443501a5934919a8b8b4f73baae800",
      "2585279f2fc74e72ac6b00b89dfa2d85",
      "de670ae3e4794034867dbf3dc50357ce",
      "decda58037af462da53bef20ff395f07"
     ]
    },
    "executionInfo": {
     "elapsed": 44713,
     "status": "ok",
     "timestamp": 1682280015832,
     "user": {
      "displayName": "NG KOK TENG",
      "userId": "06944301600455144405"
     },
     "user_tz": -120
    },
    "id": "hFV6FeVHpaFB",
    "outputId": "e1c8c8f9-c135-4be2-ac1e-89314312e83f"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/27414 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3046 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification, AdamW\n",
    "\n",
    "PRE_TRAINED_MODEL_NAME = 'xlm-roberta-base'\n",
    "\n",
    "train = pd.DataFrame({'text': train_text, 'label': train_label})\n",
    "val = pd.DataFrame({'text': val_text, 'label': val_label})\n",
    "\n",
    "train = datasets.Dataset.from_pandas(train)\n",
    "val = datasets.Dataset.from_pandas(val)\n",
    "\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME) \n",
    "\n",
    "def preprocess(examples):   \n",
    "# This will tokenize the text, add padding or truncate the text to the max length of 128     \n",
    "    return tokenizer(examples['text'],truncation=True,padding='max_length',max_length=128)   \n",
    "\n",
    "train = train.map(preprocess, batched=True)\n",
    "val = val.map(preprocess, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HRaUrptVqCTk",
   "metadata": {
    "id": "HRaUrptVqCTk",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-04-29 13:14:35,105]\u001b[0m A new study created in memory with name: no-name-4089449d-e551-44af-bdb3-aa29b8b5af64\u001b[0m\n",
      "/tmp/ipykernel_3953496/1906685436.py:7: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  learning_rate = trial.suggest_loguniform(\"learning_rate\", low=1e-6, high=1e-4)\n",
      "/tmp/ipykernel_3953496/1906685436.py:8: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use :func:`~optuna.trial.Trial.suggest_float` instead.\n",
      "  weight_decay = trial.suggest_loguniform(\"weight_decay\", low=1e-6, high=1e-4)\n",
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.dense.weight', 'roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/wifo3tp01/miniconda3/envs/authentication/lib/python3.11/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6979, 'learning_rate': 3.880067842435309e-08, 'epoch': 0.04}\n"
     ]
    }
   ],
   "source": [
    "import optuna\n",
    "import torch\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "def objective(trial: optuna.Trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    learning_rate = trial.suggest_loguniform(\"learning_rate\", low=1e-6, high=1e-4)\n",
    "    weight_decay = trial.suggest_loguniform(\"weight_decay\", low=1e-6, high=1e-4)\n",
    "    num_train_epochs = trial.suggest_categorical(\"num_train_epochs\", [1, 2, 3, 5])\n",
    "    eval_steps = trial.suggest_int(\"eval_steps\", low=10, high=1000)\n",
    "    warmup_steps = trial.suggest_int(\"warmup_steps\", low=10, high=1000)\n",
    "\n",
    "    # Load BERT model and tokenizer\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels = 2)\n",
    "\n",
    "    # Load train and validation datasets\n",
    "    train_dataset = train\n",
    "    val_dataset = val\n",
    "\n",
    "    # Define training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir='./results',\n",
    "        learning_rate=learning_rate,\n",
    "        weight_decay=weight_decay,\n",
    "        num_train_epochs=num_train_epochs,\n",
    "        per_device_train_batch_size=256,\n",
    "        per_device_eval_batch_size=256,\n",
    "        warmup_steps=warmup_steps,\n",
    "        logging_dir='./logs',\n",
    "        logging_steps=5,\n",
    "        evaluation_strategy='epoch',\n",
    "        eval_steps=eval_steps,\n",
    "        disable_tqdm=True\n",
    "    )\n",
    "\n",
    "    # Define trainer and train model\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # Return validation loss as the objective to minimize\n",
    "    return trainer.evaluate(val_dataset)['eval_loss']\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials = 20)\n",
    "    print(f\"Best trial: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "40a05751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'learning_rate': 8.71381973717022e-05, 'weight_decay': 1.777796828983435e-06, 'num_train_epochs': 3, 'eval_steps': 438, 'warmup_steps': 647}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best trial: {study.best_trial.params}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "W_wS0RMVnou6",
   "metadata": {
    "id": "W_wS0RMVnou6"
   },
   "source": [
    "# XLM-Roberta Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "FiJJKl7IYLH5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9982,
     "status": "ok",
     "timestamp": 1682280769423,
     "user": {
      "displayName": "NG KOK TENG",
      "userId": "06944301600455144405"
     },
     "user_tz": -120
    },
    "id": "FiJJKl7IYLH5",
    "outputId": "a6f05890-73a1-48e9-961c-a64b222c3e5e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained XLM-RoBERTa model and tokenizer\n",
    "model = XLMRobertaForSequenceClassification.from_pretrained(PRE_TRAINED_MODEL_NAME, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8c2eedbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "MAX_LEN = 128\n",
    "\n",
    "#Since the input data is too large that consisted by 33845 rows of sentences then we could set the batch size with small value as 16\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "train_data_loader = create_data_loader(train_text, train_label, tokenizer=tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE, sampler=RandomSampler)\n",
    "val_data_loader = create_data_loader(val_text, val_label, tokenizer=tokenizer, max_len=MAX_LEN, batch_size=BATCH_SIZE, sampler=SequentialSampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3f06c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "#Executing Stochastic Gradient Descent for adjusting the weight based on the error that calculated by using the actual label and predicted label\n",
    "#The learning rate for Stochastic Gradient Descent is 0.000001\n",
    "optimizer = AdamW(model.parameters(), lr = 8.71381973717022e-05, weight_decay = 1.777796828983435e-06,)\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Number of training epochs. The BERT authors recommend between 2 and 4 (depend on the usage, you can also set it larger)\n",
    "# We chose to run for 1 epoch first\n",
    "EPOCHS = 3\n",
    "\n",
    "# Total number of training steps is [number of batches] x [number of epochs]. \n",
    "total_steps = len(train_data_loader) * EPOCHS\n",
    "\n",
    "# Create the learning rate scheduler, here we use a linear scheduler with no warmup steps\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                            num_warmup_steps = 647,\n",
    "                                            num_training_steps = total_steps)\n",
    "\n",
    "# Define our loss function\n",
    "loss_fn = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "# Evaluation\n",
    "def eval_model(model, data_loader, loss_fn, device, n_examples):\n",
    "    model.eval()\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 438\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"labels\"].to(device)\n",
    "            outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            token_type_ids=None,\n",
    "            attention_mask=attention_mask,\n",
    "            labels=labels\n",
    "            )\n",
    "            total_eval_loss += outputs[0].item()\n",
    "            logits = outputs[1].detach().cpu().numpy()\n",
    "            label_ids = labels.to('cpu').numpy()\n",
    "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    avg_val_accuracy = total_eval_accuracy / len(data_loader)\n",
    "    avg_val_loss = total_eval_loss / len(data_loader)\n",
    "    #print(\"Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "    #print(\"Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    return avg_val_accuracy, avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "acbFk5D7lm34",
   "metadata": {
    "id": "acbFk5D7lm34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/3\n",
      "----------\n",
      "Batch: 40  of  108\n",
      "Batch: 80  of  108\n",
      "Train loss: 0.61481370638918, Accuracy: 0.626367845117845\n",
      "Test loss: 0.4855686699350675, Accuracy: 0.7629189311594202\n",
      "\n",
      "Epoch: 2/3\n",
      "----------\n",
      "Batch: 40  of  108\n",
      "Batch: 80  of  108\n",
      "Train loss: 0.28994371797199603, Accuracy: 0.8843348853114478\n",
      "Test loss: 0.4135843540231387, Accuracy: 0.8353006114130435\n",
      "\n",
      "Epoch: 3/3\n",
      "----------\n",
      "Batch: 40  of  108\n",
      "Batch: 80  of  108\n",
      "Train loss: 0.2179621364231463, Accuracy: 0.9149437079124579\n",
      "Test loss: 0.31773929049571353, Accuracy: 0.8505689538043478\n",
      "\n",
      "CPU times: user 6min 10s, sys: 2.52 s, total: 6min 12s\n",
      "Wall time: 6min 25s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from collections import defaultdict\n",
    "history = defaultdict(list)\n",
    "best_accuracy = 0\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "loss_fn.to(device)\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('Epoch: {}/{}'.format(epoch+1, EPOCHS))\n",
    "    print('-' * 10)\n",
    "    train_acc, train_loss = train_epoch(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        loss_fn,\n",
    "        optimizer,\n",
    "        device,\n",
    "        scheduler,\n",
    "        len(train_text)\n",
    "    )\n",
    "    print('Train loss: {}, Accuracy: {}'.format(train_loss, train_acc))\n",
    "    val_acc, val_loss = eval_model(\n",
    "        model,\n",
    "        val_data_loader,\n",
    "        loss_fn,\n",
    "        device,\n",
    "        len(val_text)\n",
    "    )\n",
    "    print('Test loss: {}, Accuracy: {}'.format(val_loss, val_acc))\n",
    "    print()\n",
    "    history['train_acc'].append(train_acc)\n",
    "    history['train_loss'].append(train_loss)\n",
    "    history['validation_acc'].append(val_acc)\n",
    "    history['validation_loss'].append(val_loss)\n",
    "    if val_acc > best_accuracy:\n",
    "        torch.save(model.state_dict(), 'best_model_state.bin')\n",
    "        best_accuracy = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "42ba3a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('xlm-r model (subtask 1)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1481e063",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XLMRobertaForSequenceClassification.from_pretrained('xlm-r model (subtask 1)')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b9d437aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlm_test = test.copy()\n",
    "\n",
    "# Tokenize test data\n",
    "test_inputs = tokenizer(\n",
    "    xlm_test[\"text\"].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=128,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "batch_size = 256\n",
    "num_samples = len(test_inputs[\"input_ids\"])\n",
    "test_preds = []\n",
    "\n",
    "# iterate through batches and make predictions\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    batch_inputs = {\n",
    "        \"input_ids\": test_inputs[\"input_ids\"][i:i+batch_size].to(device),\n",
    "        \"attention_mask\": test_inputs[\"attention_mask\"][i:i+batch_size].to(device)\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        batch_outputs = model(batch_inputs[\"input_ids\"], batch_inputs[\"attention_mask\"])\n",
    "        batch_preds = torch.argmax(batch_outputs.logits, dim=1)\n",
    "        test_preds.extend(batch_preds.cpu().tolist())\n",
    "\n",
    "# concatenate predictions from all batches\n",
    "xlm_test[\"label\"] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "xc47mkXFlm_k",
   "metadata": {
    "id": "xc47mkXFlm_k"
   },
   "outputs": [],
   "source": [
    "xlm_test.drop(['text'], axis = 1, inplace = True)\n",
    "xlm_test.loc[xlm_test['label'] == 0, 'label'] = 'generated'\n",
    "xlm_test.loc[xlm_test['label'] == 1, 'label'] = 'human'\n",
    "xlm_test.set_index('id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "vvu0_FhHlnEf",
   "metadata": {
    "id": "vvu0_FhHlnEf"
   },
   "outputs": [],
   "source": [
    "xlm_test.to_csv('AuTexTificationEval/task_submissions/submissions/my_team/subtask_1/en/pred (xlm-r subtask 1).tsv', sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c35813a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15725</td>\n",
       "      <td>It has remained one of my favorite country/swi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>17108</td>\n",
       "      <td>Even with very light use (hard to get motivate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>383</td>\n",
       "      <td>She died in 2015 at age 93. She is survived by...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7809</td>\n",
       "      <td>Londonderry Crown Court heard how Heaney false...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6215</td>\n",
       "      <td>Will Genia, Lachie Turner and Berrick Barnes e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text\n",
       "0  15725  It has remained one of my favorite country/swi...\n",
       "1  17108  Even with very light use (hard to get motivate...\n",
       "2    383  She died in 2015 at age 93. She is survived by...\n",
       "3   7809  Londonderry Crown Court heard how Heaney false...\n",
       "4   6215  Will Genia, Lachie Turner and Berrick Barnes e..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sub_1_test = pd.read_csv('test.tsv', sep = '\\t')\n",
    "display(sub_1_test.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c7b48ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize test data\n",
    "test_inputs = tokenizer(\n",
    "    sub_1_test[\"text\"].tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=128,\n",
    "    padding=\"max_length\",\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "batch_size = 256\n",
    "num_samples = len(test_inputs[\"input_ids\"])\n",
    "test_preds = []\n",
    "\n",
    "# iterate through batches and make predictions\n",
    "for i in range(0, num_samples, batch_size):\n",
    "    batch_inputs = {\n",
    "        \"input_ids\": test_inputs[\"input_ids\"][i:i+batch_size].to(device),\n",
    "        \"attention_mask\": test_inputs[\"attention_mask\"][i:i+batch_size].to(device)\n",
    "    }\n",
    "    with torch.no_grad():\n",
    "        batch_outputs = model(batch_inputs[\"input_ids\"], batch_inputs[\"attention_mask\"])\n",
    "        batch_preds = torch.argmax(batch_outputs.logits, dim=1)\n",
    "        test_preds.extend(batch_preds.cpu().tolist())\n",
    "\n",
    "# concatenate predictions from all batches\n",
    "sub_1_test[\"label\"] = test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54fee0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_1_test.drop(['text'], axis = 1, inplace = True)\n",
    "sub_1_test.loc[sub_1_test['label'] == 0, 'label'] = 'generated'\n",
    "sub_1_test.loc[sub_1_test['label'] == 1, 'label'] = 'human'\n",
    "sub_1_test.set_index('id', inplace = True)\n",
    "sub_1_test.to_csv('AuTexTificationEval/task_submissions/submissions/my_team/subtask_1/en/run1 (xlm-r).tsv', sep=\"\\t\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00492269a6e944109537c73c562e3432": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "02443501a5934919a8b8b4f73baae800": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "04167c969142405bb0d754fc01bdcb6b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fb9087bdada9477691cbc4c4cd38b2e0",
      "placeholder": "​",
      "style": "IPY_MODEL_c21953075c514b9ca632347105d2ff82",
      "value": "Map: 100%"
     }
    },
    "0ba494e7fa364dae93196c13ba0144c3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_04167c969142405bb0d754fc01bdcb6b",
       "IPY_MODEL_4de519c880024ba58f22ce5116a95178",
       "IPY_MODEL_d1db0ccc0c3c494690da726828d6705f"
      ],
      "layout": "IPY_MODEL_f1b69d16f76a43c7b4e37ab618824ba0"
     }
    },
    "101bde3c0df94217ba6b1615c211d940": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1c6e0f7004d54fe394ca0afb85a35383": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_263214a51f0e485fa8c8ea593e0ac76a",
       "IPY_MODEL_b473a2b03ecf422881d04ac476e92573",
       "IPY_MODEL_7a75ff77a75f45208674285b0eee9485"
      ],
      "layout": "IPY_MODEL_89a59a27e4cb4509a086181edfcdbcf1"
     }
    },
    "1fb417ca34dc45159aa389e439fbf76e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "20434d509bd04840bc04fac21a78b7af": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "2585279f2fc74e72ac6b00b89dfa2d85": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "263214a51f0e485fa8c8ea593e0ac76a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a7756b2e8044da7bb4061d0f80eb5ae",
      "placeholder": "​",
      "style": "IPY_MODEL_6911acceff924d98bb401626acb272e7",
      "value": "Map: 100%"
     }
    },
    "26b5bda23f9b4cf58f5c24e09e0c32ff": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d865d67baea4844ad5760719f532c3a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d4dd7dfcef994f48a2cd58bb13a2c064",
       "IPY_MODEL_6181eea583f0425e88b106f06362966f",
       "IPY_MODEL_73285c1ef82f4eed8ebf6857d12d068c"
      ],
      "layout": "IPY_MODEL_89df390f4d3c419ba58b25999dc7d59e"
     }
    },
    "49ef9f5a74cd41d09192e5752a4c4f21": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4de519c880024ba58f22ce5116a95178": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_26b5bda23f9b4cf58f5c24e09e0c32ff",
      "max": 3385,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c6a485c9c5df41368dc734414cbcc86b",
      "value": 3385
     }
    },
    "5db031a1c6c34bbda3e1aa56125a5419": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6181eea583f0425e88b106f06362966f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_101bde3c0df94217ba6b1615c211d940",
      "max": 30460,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_fef9d31373c745a98c142273292e87d6",
      "value": 30460
     }
    },
    "6911acceff924d98bb401626acb272e7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "73285c1ef82f4eed8ebf6857d12d068c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7582db9de64f475c8ec37d1927a3ee70",
      "placeholder": "​",
      "style": "IPY_MODEL_ab7c027542454bdfbb5a554cee3cd729",
      "value": " 30460/30460 [00:42&lt;00:00, 817.11 examples/s]"
     }
    },
    "7582db9de64f475c8ec37d1927a3ee70": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "775ddbf71fae4ad3b5d1258401ad706e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_88b10b31cfc948ce9087841b71066e8e",
       "IPY_MODEL_a9429551fb9c43ff9ab32ba7948e3891",
       "IPY_MODEL_c68286244b354d47b7d673e781ecf420"
      ],
      "layout": "IPY_MODEL_ae17f8e3c2d546aab05238a6501606ac"
     }
    },
    "7a75ff77a75f45208674285b0eee9485": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b0547766af2b4bd9b64f358d5039177c",
      "placeholder": "​",
      "style": "IPY_MODEL_20434d509bd04840bc04fac21a78b7af",
      "value": " 30460/30460 [00:39&lt;00:00, 1339.22 examples/s]"
     }
    },
    "7a7756b2e8044da7bb4061d0f80eb5ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7c7deaf37ba34db0993794e9a43b39bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "88b10b31cfc948ce9087841b71066e8e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7c7deaf37ba34db0993794e9a43b39bb",
      "placeholder": "​",
      "style": "IPY_MODEL_5db031a1c6c34bbda3e1aa56125a5419",
      "value": "Map: 100%"
     }
    },
    "89a59a27e4cb4509a086181edfcdbcf1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "89df390f4d3c419ba58b25999dc7d59e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "903a6925e28949d4b8c75e80d95800a9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a58d3ae47a774fafa87377803147fab6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a9429551fb9c43ff9ab32ba7948e3891": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_02443501a5934919a8b8b4f73baae800",
      "max": 3385,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2585279f2fc74e72ac6b00b89dfa2d85",
      "value": 3385
     }
    },
    "ab7c027542454bdfbb5a554cee3cd729": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ae17f8e3c2d546aab05238a6501606ac": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "b0547766af2b4bd9b64f358d5039177c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b3f56b71464842ac8832425afbf8253a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b473a2b03ecf422881d04ac476e92573": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_49ef9f5a74cd41d09192e5752a4c4f21",
      "max": 30460,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_1fb417ca34dc45159aa389e439fbf76e",
      "value": 30460
     }
    },
    "c21953075c514b9ca632347105d2ff82": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c68286244b354d47b7d673e781ecf420": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de670ae3e4794034867dbf3dc50357ce",
      "placeholder": "​",
      "style": "IPY_MODEL_decda58037af462da53bef20ff395f07",
      "value": " 3385/3385 [00:02&lt;00:00, 1214.52 examples/s]"
     }
    },
    "c6a485c9c5df41368dc734414cbcc86b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d1db0ccc0c3c494690da726828d6705f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_903a6925e28949d4b8c75e80d95800a9",
      "placeholder": "​",
      "style": "IPY_MODEL_b3f56b71464842ac8832425afbf8253a",
      "value": " 3385/3385 [00:04&lt;00:00, 629.40 examples/s]"
     }
    },
    "d4dd7dfcef994f48a2cd58bb13a2c064": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a58d3ae47a774fafa87377803147fab6",
      "placeholder": "​",
      "style": "IPY_MODEL_00492269a6e944109537c73c562e3432",
      "value": "Map: 100%"
     }
    },
    "de670ae3e4794034867dbf3dc50357ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "decda58037af462da53bef20ff395f07": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f1b69d16f76a43c7b4e37ab618824ba0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": "hidden",
      "width": null
     }
    },
    "fb9087bdada9477691cbc4c4cd38b2e0": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fef9d31373c745a98c142273292e87d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
